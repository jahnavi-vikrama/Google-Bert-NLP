## Course Learning
*Gained a comprehensive understanding of Google BERT's architecture, including pre-training tasks like Masked Language Modeling (MLM) and Next Sentence Prediction (NSP), as well as foundational transformer concepts such as self-attention and multi-head attention.
*Developed the ability to effectively apply and fine-tune pretrained BERT models for a range of NLP tasks, including sentiment analysis, Named Entity Recognition (NER), question answering, and domain-specific applications.
*Acquired familiarity with various BERT variants, including ALBERT, RoBERTa, and ELECTRA, and gained expertise in using lightweight models enabled by knowledge distillation (e.g., DistilBERT, TinyBERT).
*Gained practical knowledge in advanced BERT applications, including text summarization using BERTSUM, multilingual models like M-BERT, and multimodal tools such as VideoBERT.
*Successfully built real-world projects using BERT libraries, particularly Hugging Face Transformers, and applied domain-specific models like BioBERT and FinBERT for specialized applications.

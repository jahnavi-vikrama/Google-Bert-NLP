## Course Learning
<br />Gained a comprehensive understanding of Google BERT's architecture, including pre-training tasks like Masked Language Modeling (MLM) and Next Sentence Prediction (NSP), as well as foundational transformer concepts such as self-attention and multi-head attention.
<br />Developed the ability to effectively apply and fine-tune pretrained BERT models for a range of NLP tasks, including sentiment analysis, Named Entity Recognition (NER), question answering, and domain-specific applications.
<br />Acquired familiarity with various BERT variants, including ALBERT, RoBERTa, and ELECTRA, and gained expertise in using lightweight models enabled by knowledge distillation (e.g., DistilBERT, TinyBERT).
<br />Gained practical knowledge in advanced BERT applications, including text summarization using BERTSUM, multilingual models like M-BERT, and multimodal tools such as VideoBERT.
<br />Successfully built real-world projects using BERT libraries, particularly Hugging Face Transformers, and applied domain-specific models like BioBERT and FinBERT for specialized applications.
